<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Data Science Kitchen


  | Christopher

</title>
<meta name="description" content="Data Science Kitchen - Machine learning, delivered fresh from the oven!
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ”¥</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/members/christopher/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       Data Science Kitchen
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/members/">
                members
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <nav aria-label="breadcrumb">
  <ol class="breadcrumb p-0 text-lowercase">
    <li class="breadcrumb-item"><a href="/">home</a></li>
    <li class="breadcrumb-item"><a href="/members">members</a></li>
    <li class="breadcrumb-item active">Christopher</li>
  </ol>
</nav>

<div class="text-center row m-0" style="width: 100%;">
  <div class="col-sm-12 p-0" text-center>
    <img class="rounded-circle" src="/assets/img/dummy-profile.png" style="width:30%">
  </div>
  <div class="col-sm-12 p-0 text-center">
    <br/>
    <h1>Christopher <span class="font-weight-bold">Schymura</span></h1>
  </div>
  <div class="col-sm-12 p-0 text-center">
    <span class="navbar-brand social fa-2x">
      
        <a href="mailto:christopher [dot] schymura [at] data-science-kitchen [dot] de"><i class="fa fa-envelope-square"></i></a>
      
      
      
        <a href="https://www.linkedin.com/in/cschymura" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
      
      
        <a href="https://www.github.com/" target="_blank" title="GitHub"><i class="fab fa-github-square"></i></a>
      
      
        <a href="https://scholar.google.com/citations?user=_4-LU5UAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar-square"></i></a>
      
      
        <a href="https://www.researchgate.net/profile/Christopher-Schymura/" target="_blank" title="ResearchGate"><i class="ai ai-researchgate-square"></i></a>
      
      
        <a href="https://orcid.org/0000-0002-6128-3556" target="_blank" title="ORCID"><i class="ai ai-orcid-square"></i></a>
      
    </span>
    <br/><br/>
  </div>  
</div>

<div class="container-fluid p-0 text-justify">
  <h1 class="title mb-4 p-0">in a nutshell</h1>
  <p>Christopher is an experienced data scientist and machine learning researcher with a background in signal processing, probabilistic modeling and deep learning. He holds a PhD in electrical engineering and information technology from <a href="https://etit.ruhr-uni-bochum.de/" target="_blank">Ruhr University Bochum</a> and has previously worked there as a doctoral and post-doctoral researcher in the <a href="https://cognitive-signal-processing.de/" target="_blank">Cognitive Signal Processing Group</a>. Currently, he works as a data scientist in the Cognitive Solutions team at <a href="https://digital.evonik.com/" target="_blank">Evonik Digital GmbH</a>. His research interests particularly focus on uncertainty quantification for deep learning models, probabilistic machine learning and deep generative models.</p>


</div>


<div class="container-fluid p-0 text-justify">
  <h1 class="title mb-4 p-0">publications</h1>
  <div class="publications">
    
      <h2 class="year">2021</h2>
      <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICASSP</abbr>
    
  
  </div>

  <div id="Wissing2021" class="col-sm-8">
    
      <div class="title">Data Fusion for Audiovisual Speaker Localization: {E}xtending Dynamic Stream Weights to the Spatial Domain</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Wissing, Julio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  BÃ¶nninghoff, Benedikt,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kolossa, Dorothea,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ochiai, Tsubasa,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Delcroix, Marc,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kinoshita, Keisuke,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Nakatani, Tomohiro,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Araki, Shoko,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Schymura, Christopher
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2102.11588" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/rub-ksv/spatial-stream-weights" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Estimating the positions of multiple speakers can be helpful for tasks like automatic speech recognition or speaker diarization. Both applications benefit from a known speaker position when, for instance, applying beamforming or assigning unique speaker identities. Recently, several approaches utilizing acoustic signals augmented with visual data have been proposed for this task. However, both the acoustic and the visual modality may be corrupted in specific spatial regions, for instance due to poor lighting conditions or to the presence of background noise. This paper proposes a novel audiovisual data fusion framework for speaker localization by assigning individual dynamic stream weights to specific regions in the localization space. This fusion is achieved via a neural network, which combines the predictions of individual audio and video trackers based on their time- and location-dependent reliability. A performance evaluation using audiovisual recordings yields promising results, with the proposed fusion approach outperforming all baseline models.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li></ol>
    
      <h2 class="year">2020</h2>
      <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EUSIPCO</abbr>
    
  
  </div>

  <div id="Schymura2020c" class="col-sm-8">
    
      <div class="title">Exploiting Attention-based Sequence-to-Sequence Architectures for Sound Event Localization</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Schymura, Christopher,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ochiai, Tsubasa,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Delcroix, Marc,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kinoshita, Keisuke,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Nakatani, Tomohiro,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Araki, Shoko,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kolossa, Dorothea
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In European Signal Processing Conference (EUSIPCO)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2103.00417" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/document/9287224" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/chrschy/adrenaline" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Sound event localization frameworks based on deep neural networks have shown increased robustness with respect to reverberation and noise in comparison to classical parametric approaches. In particular, recurrent architectures that incorporate temporal context into the estimation process seem to be well-suited for this task. This paper proposes a novel approach to sound event localization by utilizing an attention-based sequence-to-sequence model. These types of models have been successfully applied to problems in natural language processing and automatic speech recognition. In this work, a multi-channel audio signal is encoded to a latent representation, which is subsequently decoded to a sequence of estimated directions-of-arrival. Herein, attentions allow for capturing temporal dependencies in the audio signal by focusing on specific frames that are relevant for estimating the activity and direction-of-arrival of sound events at the current time-step. The framework is evaluated on three publicly available datasets for sound event localization. It yields superior localization performance compared to state-of-the-art methods in both anechoic and reverberant conditions.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IJCNN</abbr>
    
  
  </div>

  <div id="Freiwald2020" class="col-sm-8">
    
      <div class="title">Loss Functions for Deep Monaural Speech Enhancement</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Freiwald, Jan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  SchÃ¶nherr, Lea,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Schymura, Christopher,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zeiler, Steffen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kolossa, Dorothea
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Joint Conference on Neural Networks (IJCNN)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_WCCI_2020/IJCNN/Papers/N-21276.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Deep neural networks have proven highly effective at speech enhancement, which makes them attractive not just as front-ends for machine listening and speech recognition, but also as enhancement models for the benefit of human listeners. They are, however, usually being trained on loss functions that only assess quality in terms of a minimum mean squared error. This is neglecting the fact that human audio perception functions in a manner far better described by logarithmic measures than linear ones, that psychoacoustic hearing thresholds limit the perceptibility of many signal components in a mixture, and that a degree of continuity of signals may also be expected. Hence, sudden changes in the gain of a system may be detrimental. In the following, we cast these properties of human perception into a form that can aid the optimization of a deep neural network speech enhancement system. We explore their effects on a range of model topologies, showing the efficacy of the proposed modifications.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICASSP</abbr>
    
  
  </div>

  <div id="Schymura2020b" class="col-sm-8">
    
      <div class="title">A Dynamic Stream Weight Backprop Kalman Filter for Audiovisual Speaker Tracking</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Schymura, Christopher,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ochiai, Tsubasa,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Delcroix, Marc,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kinoshita, Keisuke,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Nakatani, Tomohiro,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Araki, Shoko,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kolossa, Dorothea
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/document/9054005" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Audiovisual speaker tracking is an application that has been tackled by a wide range of classical approaches based on Gaussian filters, most notably the well-known Kalman filter. Recently, a specific Kalman filter implementation was proposed for this task, which incorporated dynamic stream weights to explicitly control the influence of acoustic and visual observations during estimation. Inspired by recent progress in the context of integrating uncertainty estimates into modern deep learning frameworks, this paper proposes a deep neural-network-based implementation of the Kalman filter with dynamic stream weights, whose parameters can be learned via standard backpropagation. This allows for jointly optimizing the parameters of the model and the dynamic stream weight estimator in a unified framework. An experimental study on audiovisual speaker tracking shows that the proposed model shows comparable performance to state-of-the-art recurrent neural networks with the additional advantage of requiring a smaller number of parameters and providing explicit uncertainty information.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">TASLP</abbr>
    
  
  </div>

  <div id="Schymura2020a" class="col-sm-8">
    
      <div class="title">Audiovisual Speaker Tracking using Nonlinear Dynamical Systems with Dynamic Stream Weights</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Schymura, Christopher,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kolossa, Dorothea
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/document/9037104" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Data fusion plays an important role in many technical applications that require efficient processing of multimodal sensory observations. A prominent example is audiovisual signal processing, which has gained increasing attention in automatic speech recognition, speaker localization and related tasks. If appropriately combined with acoustic information, additional visual cues can help to improve the performance in these applications, especially under adverse acoustic conditions. A dynamic weighting of acoustic and visual streams based on instantaneous sensor reliability measures is an efficient approach to data fusion in this context. This article presents a framework that extends the well-established theory of nonlinear dynamical systems with the notion of dynamic stream weights for an arbitrary number of sensory observations. It comprises a recursive state estimator based on the Gaussian filtering paradigm, which incorporates dynamic stream weights into a framework closely related to the extended Kalman filter. Additionally, a convex optimization approach to estimate oracle dynamic stream weights in fully observed dynamical systems utilizing a Dirichlet prior is presented. This serves as a basis for a generic parameter learning framework of dynamic stream weight estimators. The proposed system is application-independent and can be easily adapted to specific tasks and requirements. A study using audiovisual speaker tracking tasks is considered as an exemplary application in this work. An improved tracking performance of the dynamic stream-weight-based estimation framework over state-of-the-art methods is demonstrated in the experiments.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">TASLP</abbr>
    
  
  </div>

  <div id="Trowitzsch2020" class="col-sm-8">
    
      <div class="title">Joining Sound Event Detection and Localization Through Spatial Segregation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Trowitzsch, Ivo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Schymura, Christopher,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kolossa, Dorothea,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Obermayer, Klaus
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/document/8928942" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Identification and localization of sounds are both integral parts of computational auditory scene analysis. Although each can be solved separately, the goal of forming coherent auditory objects and achieving a comprehensive spatial scene understanding suggests pursuing a joint solution of the two problems. This article presents an approach that robustly binds localization with the detection of sound events in a binaural robotic system. Both tasks are joined through the use of spatial stream segregation which produces probabilistic time-frequency masks for individual sources attributable to separate locations, enabling segregated sound event detection operating on these streams. We use simulations of a comprehensive suite of test scenes with multiple co-occurring sound sources, and propose performance measures for systematic investigation of the impact of scene complexity on this segregated detection of sound types. Analyzing the effect of spatial scene arrangement, we show how a robot could facilitate high performance through optimal head rotation. Furthermore, we investigate the performance of segregated detection given possible localization error as well as error in the estimation of number of active sources. Our analysis demonstrates that the proposed approach is an effective method to obtain joint sound event location and type information under a wide range of conditions.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li></ol>
    
      <h2 class="year">2019</h2>
      <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICASSP</abbr>
    
  
  </div>

  <div id="Schymura2019" class="col-sm-8">
    
      <div class="title">Learning Dynamic Stream Weights for Linear Dynamical Systems Using Natural Evolution Strategies</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Schymura, Christopher,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kolossa, Dorothea
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/document/8682249" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Multimodal data fusion is an important aspect of many object localization and tracking frameworks that rely on sensory observations from different sources. A prominent example is audiovisual speaker localization, where the incorporation of visual information has shown to benefit overall performance, especially in adverse acoustic conditions. Recently, the notion of dynamic stream weights as an efficient data fusion technique has been introduced into this field. Originally proposed in the context of audiovisual automatic speech recognition, dynamic stream weights allow for effective sensory-level data fusion on a per-frame basis, if reliability measures for the individual sensory streams are available. This study proposes a learning framework for dynamic stream weights based on natural evolution strategies, which does not require the explicit computation of oracle information. An experimental evaluation based on recorded audiovisual sequences shows that the proposed approach outperforms conventional methods based on supervised training in terms of localization performance.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li></ol>
    
      <h2 class="year">2018</h2>
      <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EUSIPCO</abbr>
    
  
  </div>

  <div id="Ito2018" class="col-sm-8">
    
      <div class="title">Noisy {cGMM}: {C}omplex {G}aussian Mixture Model with Non-Sparse Noise Model for Joint Source Separation and Denoising</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Ito, Nobutaka,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Schymura, Christopher,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Araki, Shoko,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Nakatani, Tomohiro
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In European Signal Processing Conference (EUSIPCO)</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/document/8553410" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Here we introduce a noisy cGMM, a probabilistic model for noisy, mixed signals observed by a microphone array for joint source separation and denoising. In a conventional time-varying complex Gaussian mixture model (cGMM), the observed signals are assumed to be composed of sparse target signals only, where the sparseness refers to the property of having significant power at only a few time-frequency points. However, this assumption becomes inaccurate in the presence of non-sparse signals such as background noise, which renders speech enhancement based on the cGMM less effective. In contrast, the proposed noisy cGMM is based on the assumption that the observed signals consist of not only sparse target signals but also non-sparse background noise. This enables the noisy cGMM to model the observed signals accurately even in the presence of non-sparse background noise, which leads to effective speech enhancement. We also propose a joint diagonalization-based algorithm for estimating the model parameters of the noisy cGMM, which is significantly faster than the standard EM algorithm without any performance degradation. Indeed, the joint diagonalization bypasses the need for matrix inversion, matrix multiplication, and determinant computation at each time-frequency point, which are needed in the EM algorithm. In an experiment, the noisy cGMM outperformed the cGMM in joint source separation and denoising.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IWAENC</abbr>
    
  
  </div>

  <div id="Schymura2018c" class="col-sm-8">
    
      <div class="title">Extending Linear Dynamical Systems with Dynamic Stream Weights for Audiovisual Speaker Localization</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Schymura, Christopher,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Isenberg, Tobias,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kolossa, Dorothea
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Workshop on Acoustic Signal Enhancement (IWAENC)</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/document/8521384" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>An important aspect of audiovisual speaker localization is the appropriate fusion of acoustic and visual observations based on their time-varying reliability. In this study, a framework which incorporates dynamic stream weights into the well-known Kalman filtering framework is proposed to cope with this challenge. The concept of dynamic stream weights has recently been investigated in the context of audiovisual automatic speech recognition, where it was successfully applied to weight audiovisual observations according to their reliability. This study extends that approach to linear dynamical systems and additionally introduces a closed-form solution to compute oracle dynamic stream weights from observation sequences with known state trajectories. The proposed approach is evaluated on audiovisual recordings from a humanoid robot in reverberant environments. The results indicate that incorporating dynamic stream weights allows for efficient data fusion on a per-frame basis, which shows superior performance over conventional Kalman-filter-based state estimation.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">LVA/ICA</abbr>
    
  
  </div>

  <div id="Schymura2018b" class="col-sm-8">
    
      <div class="title">Exploiting Structures of Temporal Causality for Robust Speaker Localization in Reverberant Environments</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Schymura, Christopher,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Guo, Peng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Maymon, Yanir,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Rafaely, Boaz,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kolossa, Dorothea
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Latent Variable Analysis and Signal Separation (LVA/ICA)</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://link.springer.com/chapter/10.1007/978-3-319-93764-9_22" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper introduces a framework for robust speaker localization in reverberant environments based on a causal analysis of the temporal relationship between direct sound and corresponding reflections. It extends previously proposed localization approaches for spherical microphone arrays based on a direct-path dominance test. So far, these methods are applied in the time-frequency domain without considering the temporal context of direction-of-arrival measurements. In this work, a causal analysis of the temporal structure of subsequent directions-of-arrival estimates based on the Granger causality test is proposed. The cause-effect relationship between estimated directions is modeled via a causal graph, which is used to distinguish the direction of the direct sound from corresponding reflections. An experimental evaluation in simulated acoustic environments shows that the proposed approach yields an improvement in localization performance especially in highly reverberant conditions.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICASSP</abbr>
    
  
  </div>

  <div id="Schymura2018a" class="col-sm-8">
    
      <div class="title">Potential-Field-Based Active Exploration for Acoustic Simultaneous Localization and Mapping</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Schymura, Christopher,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kolossa, Dorothea
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/document/8461655" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper presents a novel framework for active exploration in the context of acoustic simultaneous localization and mapping (SLAM) using a microphone array mounted on a mobile robotic agent. Acoustic SLAM aims at building a map of acoustic sources present in the environment and simultaneously estimating the agent's own trajectory and position within this map. Two important aspects of this task are robustness against disturbances arising from reverberation and sensor imperfections and an appropriate degree of exploration to achieve high map accuracy. Several approaches to the latter aspect using information-theoretic measures have recently been proposed. This study extends these approaches into a framework based on the potential field method, which is a widely used technique for robotic path planning and navigation. It allows to determine exploratory movement trajectories for the robotic agent via gradient descent, without requiring computationally expensive Monte Carlo simulations to predict the effects of specific trajectory choices. Furthermore, additional constraints like maintaining a safe distance to acoustic sources can easily be integrated into this framework. Experimental evaluation demonstrates that the proposed method yields adequate exploration strategies of the acoustic environment leading to accurate map estimates.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li></ol>
    
      <h2 class="year">2017</h2>
      <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICASSP</abbr>
    
  
  </div>

  <div id="Meutzner0217" class="col-sm-8">
    
      <div class="title">Improving Audio-Visual Speech Recognition using Deep Neural Networks with Dynamic Stream Reliability Estimates</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Meutzner, Hendrik,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ma, Ning,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Nickel, Robert,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Schymura, Christopher,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kolossa, Dorothea
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/document/7953172" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Audio-visual speech recognition is a promising approach to tackling the problem of reduced recognition rates under adverse acoustic conditions. However, finding an optimal mechanism for combining multi-modal information remains a challenging task. Various methods are applicable for integrating acoustic and visual information in Gaussian-mixture-model-based speech recognition, e.g., via dynamic stream weighting. The recent advances of deep neural network (DNN)-based speech recognition promise improved performance when using audio-visual information. However, the question of how to optimally integrate acoustic and visual information remains. In this paper, we propose a state-based integration scheme that uses dynamic stream weights in DNN-based audio-visual speech recognition. The dynamic weights are obtained from a time-variant reliability estimate that is derived from the audio signal. We show that this state-based integration is superior to early integration of multi-modal features, even if early integration also includes the proposed reliability estimate. Furthermore, the proposed adaptive mechanism is able to outperform a fixed weighting approach that exploits oracle knowledge of the true signal-to-noise ratio.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICASSP</abbr>
    
  
  </div>

  <div id="Schymura2017" class="col-sm-8">
    
      <div class="title">Monte Carlo Exploration for Active Binaural Localization</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Schymura, Christopher,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  {Rios Grajales}, Juan Diego,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kolossa, Dorothea
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/document/7952204" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This study introduces a machine hearing system for robot audition, which enables a robotic agent to pro-actively minimize the uncertainty of sound source location estimates through motion. The proposed system is based on an active exploration approach, providing a means to model and predict effects of the agent's future motions on localization uncertainty in a probabilistic manner. Particle filtering is used to estimate the posterior probability density function of the source position from binaural measurements, enabling to jointly assess azimuth and distance of the source. The framework allows to infer and refine a policy to select appropriate actions via a Monte Carlo exploration approach. Experiments in simulated reverberant conditions are conducted, showing that active exploration and the incorporation of distance estimation significantly improve localization performance.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li></ol>
    
      <h2 class="year">2015</h2>
      <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">INTERSPEECH</abbr>
    
  
  </div>

  <div id="Schymura2015" class="col-sm-8">
    
      <div class="title">Binaural Sound Source Localisation and Tracking Using a Dynamic Spherical Head Model</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Schymura, Christopher,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Winter, Fiete,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kolossa, Dorothea,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Spors, Sascha
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Annual Conference of the International Speech Communication Association (INTERSPEECH)</em>
      
      
        2015
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://www.int.uni-rostock.de/fileadmin/user_upload/publications/spors/2015/Schymura_et_al_EURONOISE_Binaural_Localization.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper introduces a binaural model for the localisation and tracking of a moving sound sourceâ€™s azimuth in the horizontal plane. The model uses a nonlinear state space representation of the sound source dynamics including the current position of the listenerâ€™s head. The state is estimated via an unscented Kalman Filter by comparing the interaural level and time differences of the binaural signal with semi-analytically derived localisation cues from a spherical head model. The localisation performance of the model is evaluated in combination with two different head movement approaches based on open-and closed-loop control strategies. The results show that adaptive strategies outperform non-adaptive ones and are able to compensate systematic deviations between the spherical head model and human heads.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li></ol>
    
  </div>
</div>


    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2021 Data Science Kitchen.
    <br />Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
    <a href="/imprint/">Impressum</a>.
    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
